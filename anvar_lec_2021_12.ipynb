{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anvar_lec_2021_12.ipynb\n",
    "# 2021.12.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas behind attention models\n",
    "\n",
    "* [slides attention.pdf](attention.pdf)\n",
    "* [YT:2021: From linear models through GCN to Attention - Anvar Kurmukov (AIRI)](https://www.youtube.com/watch?v=-hXUsOIcUqM)\n",
    "  * Lin Model $Xw=y$ \n",
    "    * $X$ is an $n\\times m$ matrix of $n$ observations/measurements and $m$ features.\n",
    "    * $w$ is $m\\times 1$ matrix (vector) of weights we want to \"learn\", and $y=t$ is $n\\times 1$ martix (vector) of target values observed in our experiments/measurements.\n",
    "    * Note: if we make prediction for $c$ classes, then the linear model can be written as $XW=Y$, where $W$ is $m\\times c$, and $Y$ is $n\\times c$. \n",
    "    * Example-1: target is a binary loan approval decision: 0 or 1. Each row corresponds to a person: Bill, Mary, Ann, etc. Each column corresponds to each person's info: age, income, education, etc.\n",
    "      * If Bill and Mary are married, we may want to pay attention to this fact in our loan approval prediction model. E.g., this can be done by in troducing a matrix $A$ that will add Bill's row to Mary's info accordingly (with cetrain weight) when decision for Mary is beng made and visa versa.\n",
    "    * Example-2: Predict a missing word in a sentence: \"I like to MASK in the winder.\" Now relations between words are important and they can be captured by a matrix $A$.\n",
    "      * $A X w = y$\n",
    "    * How to choose an Attention matrix $A$?\n",
    "    * Option 1: $A= X X'$ . See [arxiv.org/pdf/1508.04025.pdf](http://arxiv.org/pdf/1508.04025.pdf) \n",
    "    * option 2: score($h_t$ , $h_s'$) = $h_t W_s h_s$.\n",
    "      * $A = X W X'$, with $W$ being low rank.\n",
    "      * $A = X K Q' X'$.    Both $K$ and $Q$ are low rank matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped at 1:07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers\n",
    "\n",
    "* [2015, Effective Approaches to Attention-based Neural Machine Translation, Minh-Thang Luong, Hieu Pham, Christopher D. Manning](https://arxiv.org/abs/1508.04025)\n",
    "* [2017, Attention Is All You Need; Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin](https://arxiv.org/abs/1706.03762)\n",
    "* [2020, GLU Variants Improve Transformer, Noam Shazeer, arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)\n",
    "* []()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
