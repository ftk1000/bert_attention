Anvar Lecture 2021.12

# Ideas behind attention models

* [slides attention.pdf](attention.pdf)
* [YT:2021: From linear models through GCN to Attention - Anvar Kurmukov (AIRI)](https://www.youtube.com/watch?v=-hXUsOIcUqM)
  * Lin Model Xw=y 
  * A X w = y
    * How to choose A?
    * Option 1: A= X X' . See [arxiv.org/pdf/1508.04025.pdf](http://arxiv.org/pdf/1508.04025.pdf) 
    * option 2: score(h_t , h_s') = h_t W_s h_s
      * A = X W X', with W being low rank.
      * A = X K Q' X'.    Both K and Q are low rank
      

# Ideas behind attention models

* [slides attention.pdf](attention.pdf)
* [YT:2021: From linear models through GCN to Attention - Anvar Kurmukov (AIRI)](https://www.youtube.com/watch?v=-hXUsOIcUqM)
  * Lin Model $Xw=y$ 
    * $X$ is an $n\times m$ matrix of $n$ observations/measurements and $m$ features.
    * $w$ is $m\times 1$ matrix (vector) of weights we want to "learn", and $y=t$ is $n\times 1$ martix (vector) of target values observed in our experiments/measurements.
    * Note: if we make prediction for $c$ classes, then the linear model can be written as $XW=Y$, where $W$ is $m\times c$, and $Y$ is $n\times c$. 
    * Example-1: target is a binary loan approval decision: 0 or 1. Each row corresponds to a person: Bill, Mary, Ann, etc. Each column corresponds to each person's info: age, income, education, etc.
      * If Bill and Mary are married, we may want to pay attention to this fact in our loan approval prediction model. E.g., this can be done by in troducing a matrix $A$ that will add Bill's row to Mary's info accordingly (with cetrain weight) when decision for Mary is beng made and visa versa.
    * Example-2: Predict a missing word in a sentence: "I like to MASK in the winder." Now relations between words are important and they can be captured by a matrix $A$.
      * $A X w = y$
    * How to choose an Attention matrix $A$?
    * Option 1: $A= X X'$ . See [arxiv.org/pdf/1508.04025.pdf](http://arxiv.org/pdf/1508.04025.pdf) 
    * option 2: score($h_t$ , $h_s'$) = $h_t W_s h_s$.
      * $A = X W X'$, with $W$ being low rank.
      * $A = X K Q' X'$.    Both $K$ and $Q$ are low rank matrices.
      
      # Papers

* [2015, Effective Approaches to Attention-based Neural Machine Translation, Minh-Thang Luong, Hieu Pham, Christopher D. Manning](https://arxiv.org/abs/1508.04025)
* [2017, Attention Is All You Need; Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin](https://arxiv.org/abs/1706.03762)
* [2020, GLU Variants Improve Transformer, Noam Shazeer, arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)
* []()
